import streamlit as st
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, roc_auc_score, log_loss
from sklearn.model_selection import validation_curve
from sklearn.linear_model import LogisticRegression, SGDClassifier
from utils import load_session, save_session

def main():
    # Carrega sess√£o salva
    if 'dados' not in st.session_state:
        saved = load_session()
        st.session_state.update(saved)
        if saved:
            st.info("‚úÖ Dados recuperados da sess√£o anterior.")
    
    st.title("‚úÖ An√°lise e Valida√ß√£o do Modelo")
    st.markdown("""
    Entenda **como o seu modelo se comporta na pr√°tica** com m√©tricas essenciais para credit scoring.  
    Aqui voc√™ vai aprender o que cada indicador significa ‚Äî e por que ele importa.
    """)
    
    modelo_tipo = st.session_state.get('modelo_tipo', 'Desconhecido')
    target = st.session_state.get('target', 'Vari√°vel-Alvo')

    if 'modelo' not in st.session_state:
        st.warning("Nenhum modelo treinado! Construa um modelo primeiro.")
        st.page_link("pages/6_ü§ñ_Modelagem.py", label="‚Üí Ir para Modelagem", icon="ü§ñ")
        return

    X_test = st.session_state.get('X_test')
    y_test = st.session_state.get('y_test')
    features = X_test.columns if isinstance(X_test, pd.DataFrame) else range(X_test.shape[1])

    if X_test is None or y_test is None:
        st.error("Dados de teste n√£o dispon√≠veis. Treine o modelo novamente.")
        return

    model = st.session_state.modelo
    y_pred = model.predict(X_test)

    # Probabilidades (necess√°rias para ROC e KS)
    try:
        y_proba = model.predict_proba(X_test)[:, 1]  # Probabilidade de classe 1 (inadimplente)
    except:
        st.warning("O modelo n√£o suporta `predict_proba`. Usando predi√ß√£o direta.")
        y_proba = y_pred

    # --- 1. MATRIZ DE CONFUS√ÉO ---
    st.markdown("### üìä Matriz de Confus√£o")
    st.info("""
    Mostra os acertos e erros do modelo. Ajuda a entender **quem foi classificado corretamente**.
    - **VP (Verdadeiro Positivo)**: Inadimplente ‚Üí previsto como inadimplente ‚úÖ  
    - **VN (Verdadeiro Negativo)**: Adimplente ‚Üí previsto como adimplente ‚úÖ  
    - **FP (Falso Positivo)**: Adimplente ‚Üí previsto como inadimplente ‚ùå (rejei√ß√£o injusta)  
    - **FN (Falso Negativo)**: Inadimplente ‚Üí previsto como adimplente ‚ùå (pior erro: risco n√£o detectado)
    """)

    cm = confusion_matrix(y_test, y_pred)
    fig, ax = plt.subplots(figsize=(2.5, 2))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
                xticklabels=['Adimplente (0)', 'Inadimplente (1)'],
                yticklabels=['Adimplente (0)', 'Inadimplente (1)'])
    ax.set_ylabel('Real')
    ax.set_xlabel('Previsto')
    ax.set_title('Matriz de Confus√£o')
    st.pyplot(fig)

    # --- 2. RELAT√ìRIO DE CLASSIFICA√á√ÉO AMIG√ÅVEL ---
    st.markdown("### üß† Relat√≥rio de Classifica√ß√£o (Explicado)")
    st.info("""
    Abaixo, cada m√©trica √© explicada com base em um cen√°rio de concess√£o de cr√©dito.  
    Imagine que o modelo decide **a quem emprestar dinheiro**.
    """)

    # Calcula m√©tricas
    tn, fp, fn, tp = cm.ravel()
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    accuracy = (tp + tn) / (tp + tn + fp + fn)

    # Tabela de m√©tricas
    metricas_df = pd.DataFrame({
        "M√©trica": ["Precision", "Recall", "F1-Score", "Acur√°cia"],
        "Valor": [f"{precision:.2%}", f"{recall:.2%}", f"{f1:.2%}", f"{accuracy:.2%}"],
        "Explica√ß√£o": [
            "**Precision (Precis√£o)**: Entre todos os clientes marcados como 'inadimplentes', quantos realmente s√£o? Alta precis√£o = poucos falsos positivos (n√£o rejeitamos bons clientes por engano).",
            "**Recall (Sensibilidade)**: Dos verdadeiros inadimplentes, quantos o modelo conseguiu identificar? Alto recall = poucos falsos negativos (capturamos mais risco).",
            "**F1-Score**: M√©dia harm√¥nica entre Precision e Recall. √ìtimo quando queremos equil√≠brio entre capturar risco e n√£o rejeitar bons clientes.",
            "**Acur√°cia**: Propor√ß√£o total de acertos. Pode ser enganosa se a base for desbalanceada (ex: 95% adimplentes)."
        ]
    })

    for _, row in metricas_df.iterrows():
        st.markdown(f"#### {row['M√©trica']} ‚Üí `{row['Valor']}`")
        st.markdown(row['Explica√ß√£o'])
        st.markdown("---")

    # --- 3. CURVA ROC e AUROC ---
    st.markdown("### üìà Curva ROC e AUC")
    st.info("""
    A **Curva ROC** mostra o trade-off entre **verdadeiros positivos (Recall)** e **falsos positivos** em diferentes limiares.
    - **AUC (√Årea sob a curva)**: Quanto maior, melhor o modelo separa bons e maus pagadores.
    - **AUC > 0.7**: razo√°vel | **> 0.8**: bom | **> 0.9**: excelente
    """)

    fpr, tpr, _ = roc_curve(y_test, y_proba)
    roc_auc = auc(fpr, tpr)

    fig, ax = plt.subplots(figsize=(6, 5))
    ax.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC (AUC = {roc_auc:.2f})')
    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Aleat√≥rio (AUC = 0.5)')
    ax.set_xlim([0.0, 1.0])
    ax.set_ylim([0.0, 1.05])
    ax.set_xlabel('Taxa de Falsos Positivos (1 - Especificidade)')
    ax.set_ylabel('Taxa de Verdadeiros Positivos (Recall)')
    ax.set_title('Curva ROC')
    ax.legend(loc="lower right")
    st.pyplot(fig)

    st.metric("AUC (ROC)", f"{roc_auc:.2f}")

    # --- 4. TESTE KS (Kolmogorov-Smirnov) ---
    st.markdown("### üìä Estat√≠stica KS")
    st.info("""
    O **KS** mede a maior separa√ß√£o entre a distribui√ß√£o acumulada de **bons** e **maus pagadores**.
    - **KS > 0.3**: bom poder de separa√ß√£o
    - **KS > 0.4**: excelente
    √â uma m√©trica muito usada em cr√©dito porque mostra claramente o poder discriminat√≥rio do modelo.
    """)

    # Distribui√ß√£o acumulada
    thresholds = np.linspace(0, 1, 100)
    tpr_ks = [np.mean(y_proba[y_test == 1] >= th) for th in thresholds]
    fpr_ks = [np.mean(y_proba[y_test == 0] >= th) for th in thresholds]
    ks_values = [t - f for t, f in zip(tpr_ks, fpr_ks)]
    ks_max = max(ks_values)
    best_th = thresholds[np.argmax(ks_values)]

    fig, ax = plt.subplots(figsize=(6, 4))
    ax.plot(thresholds, tpr_ks, label='Bons (TPR)', color='green')
    ax.plot(thresholds, fpr_ks, label='Maus (FPR)', color='red')
    ax.plot(thresholds, ks_values, label='KS (diferen√ßa)', color='blue', linestyle='--')
    ax.axvline(best_th, color='gray', linestyle=':', label=f'Limiar √≥timo (KS): {best_th:.2f}')
    ax.set_xlabel('Limiar de decis√£o')
    ax.set_ylabel('Taxa acumulada')
    ax.set_title('An√°lise KS')
    ax.legend()
    st.pyplot(fig)

    st.metric("KS M√°ximo", f"{ks_max:.2f}")
    st.caption(f"Limiar √≥timo para KS: {best_th:.2f}")

    # --- 8. C√ÅLCULO DO CUSTO DOS ERROS (FALSOS NEGATIVOS) ---
    st.markdown("### üí∏ Custo dos Erros: Falsos Negativos")
    st.info("""
    Um **Falso Negativo (FN)** ocorre quando o modelo diz que o cliente √© **adimplente (0)**,  
    mas ele **√© inadimplente (1)**. Ou seja: **o cr√©dito foi concedido, mas o cliente n√£o pagar√°**.
    
    Assumimos aqui que o **custo desse erro √© o valor do empr√©stimo concedido (`loan_amount`)**.
    """)
    
    # Recupera os dados originais
    df_original = st.session_state.get('dados')
    if df_original is None:
        st.warning("‚ö†Ô∏è Dados brutos n√£o dispon√≠veis. N√£o √© poss√≠vel calcular o custo do erro.")
        return
    
    # Lista de colunas num√©ricas candidatas a "valor do empr√©stimo"
    colunas_numericas = df_original.select_dtypes(include=[np.number]).columns.tolist()
    
    if len(colunas_numericas) == 0:
        st.warning("Nenhuma coluna num√©rica dispon√≠vel para usar como valor do empr√©stimo.")
        return
    
    # Op√ß√£o para o usu√°rio escolher a coluna de valor
    coluna_valor = st.selectbox(
        "Selecione a coluna que representa o **valor do empr√©stimo**:",
        options=colunas_numericas,
        index=colunas_numericas.index('loan_amount') if 'loan_amount' in colunas_numericas else 0,
        help="Essa coluna ser√° usada para calcular o custo dos erros (ex: loan_amount, amount, valor_emprestimo)"
    )
    
    # Garante que y_test e X_test est√£o alinhados com o df_original
    try:
        # Extrai os valores do empr√©stimo para o conjunto de teste
        loan_test = df_original.loc[y_test.index, coluna_valor]
    
        # Cria DataFrame com real, predito e valor do empr√©stimo
        results = pd.DataFrame({
            'y_real': y_test,
            'y_pred': y_pred,
            'valor_emprestimo': loan_test.values
        })
    
        # Filtra os Falsos Negativos
        falsos_negativos = results[(results['y_real'] == 1) & (results['y_pred'] == 0)]
    
        if len(falsos_negativos) == 0:
            st.success("‚úÖ Nenhum Falso Negativo encontrado! Nenhum custo de erro.")
            custo_total = 0.0
        else:
            custo_total = falsos_negativos['valor_emprestimo'].sum()
            media_custo = falsos_negativos['valor_emprestimo'].mean()
    
            st.warning(f"‚ö†Ô∏è Foram identificados **{len(falsos_negativos)} Falsos Negativos**.")
            st.metric("Custo Total dos Falsos Negativos", f"R$ {custo_total:,.2f}")
            st.write(f"Valor m√©dio do empr√©stimo por FN: R$ {media_custo:,.2f}")
    
            # Mostra uma amostra
            with st.expander("üìä Ver detalhes dos Falsos Negativos"):
                st.dataframe(
                    falsos_negativos[['valor_emprestimo']]
                    .rename(columns={'valor_emprestimo': 'Valor do Empr√©stimo'})
                    .head(10)
                )
    
        # Armazena para uso futuro (ex: otimiza√ß√£o de limiar)
        st.session_state.custo_falsos_negativos = custo_total
        st.session_state.coluna_valor_emprestimo = coluna_valor
    
    except KeyError:
        st.error(f"‚ùå A coluna `{coluna_valor}` n√£o est√° dispon√≠vel no conjunto de teste.")
    except Exception as e:
        st.error(f"Erro ao calcular custo dos erros: {e}")
    
    # --- 9. INDICADOR RELATIVO: Custo do Erro vs. Montante Total Emprestado ---
    st.markdown("### üìä Indicador Relativo: Custo do Erro")
    st.info("""
    O custo absoluto dos erros √© importante, mas ganha mais sentido quando comparado ao **total de cr√©dito concedido**.
    
    Aqui, calculamos a **propor√ß√£o do valor dos empr√©stimos que se tornou preju√≠zo** por causa de Falsos Negativos.
    """)
    
    # Recupera o valor total emprestado no conjunto de teste
    try:
        # Valor total emprestado (soma de todos os loan_amount no conjunto de teste)
        montante_total = df_original.loc[y_test.index, coluna_valor].sum()
    
        if 'custo_falsos_negativos' in st.session_state:
            custo_fn = st.session_state.custo_falsos_negativos
        else:
            # Recalcula se necess√°rio
            results = pd.DataFrame({
                'y_real': y_test,
                'y_pred': y_pred,
                'valor': df_original.loc[y_test.index, coluna_valor].values
            })
            falsos_negativos = results[(results['y_real'] == 1) & (results['y_pred'] == 0)]
            custo_fn = falsos_negativos['valor'].sum()
    
        # Calcula o indicador relativo
        if montante_total > 0:
            percentual_perda = (custo_fn / montante_total) * 100
        else:
            percentual_perda = 0.0
    
        # Exibe os resultados
        col1, col2, col3 = st.columns(3)
        col1.metric("Montante Total Concedido", f"R$ {montante_total:,.2f}")
        col2.metric("Custo dos Falsos Negativos", f"R$ {custo_fn:,.2f}")
        col3.metric("Taxa de Perda por Erro", f"{percentual_perda:.2f}%")
    
        # Interpreta√ß√£o
        if percentual_perda < 1:
            st.success(f"‚úÖ Baixo impacto: apenas **{percentual_perda:.2f}%** do montante concedido foi perdido por erros do modelo.")
        elif percentual_perda < 5:
            st.warning(f"‚ö†Ô∏è Impacto moderado: **{percentual_perda:.2f}%** do cr√©dito concedido resultou em preju√≠zo.")
        else:
            st.error(f"üö® Alto impacto: **{percentual_perda:.2f}%** do valor emprestado foi perdido por Falsos Negativos. Reavalie o modelo ou o limiar de aprova√ß√£o.")
    
        # Armazena para relat√≥rio
        st.session_state.indicador_relativo_custo = {
            'montante_total': montante_total,
            'custo_fn': custo_fn,
            'percentual_perda': percentual_perda
        }
    
    except Exception as e:
        st.error(f"Erro ao calcular indicador relativo: {e}")
    save_session()
    
    # --- 5. AN√ÅLISE DE OVERFITTING (Curva de Perda) ---
    st.markdown("### üìâ An√°lise de Overfitting: Curva de Perda")
    st.info("""
    **Overfitting** ocorre quando o modelo "decora" os dados de treino, mas falha em generalizar para novos dados.  
    A **curva de perda** mostra o desempenho do modelo no treino e no teste ao longo do tempo (ou de itera√ß√µes).  
    Se a curva de treino continua melhorando, mas a do teste estabiliza ou piora, √© sinal de overfitting.
    """)

    # Simula√ß√£o de curva de perda (j√° que sklearn n√£o fornece diretamente)
    try:
        # Usamos um modelo similar para simular a curva (apenas para fins did√°ticos)
        if modelo_tipo == "Random Forest":
            from sklearn.ensemble import RandomForestClassifier
            model_for_curve = RandomForestClassifier(max_depth=10, random_state=42)
            param_range = np.arange(1, 11)
            train_scores, test_scores = validation_curve(
                model_for_curve, X_test, y_test, param_name="max_depth", param_range=param_range,
                cv=3, scoring="accuracy", n_jobs=-1
            )
            xlabel = "Profundidade da √Årvore (max_depth)"
            title = "Curva de Valida√ß√£o - Random Forest"
            # Gr√°fico
            fig, ax = plt.subplots(figsize=(7, 5))
            ax.plot(param_range, train_mean, color='blue', marker='o', markersize=5, label='Treino')
            ax.fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.15, color='blue')
        
            ax.plot(param_range, test_mean, color='red', marker='s', linestyle='--', markersize=5, label='Valida√ß√£o')
            ax.fill_between(param_range, test_mean - test_std, test_mean + test_std, alpha=0.15, color='red')
        
            ax.set_xlabel(xlabel)
            ax.set_ylabel('Acur√°cia')
            ax.set_title(title)
            ax.legend(loc='lower right')
            ax.grid(True)
            st.pyplot(fig)

        else:
            st.info("""
                ‚ö†Ô∏è Para modelos de **Regress√£o Log√≠stica**, n√£o geramos a curva de perda.
                
                Motivo: O `LogisticRegression` do scikit-learn **n√£o fornece a fun√ß√£o de perda por itera√ß√£o**.  
                Al√©m disso, como o modelo √© relativamente simples e convexamente otimizado, o overfitting pode ser melhor avaliado por m√©tricas de teste (acur√°cia, KS, AUC) do que por curvas de loss.
                """) 

        # M√©dia e desvio
        train_mean = np.mean(train_scores, axis=1)
        train_std = np.std(train_scores, axis=1)
        test_mean = np.mean(test_scores, axis=1)
        test_std = np.std(test_scores, axis=1)
        
        # Interpreta√ß√£o
        if np.argmax(test_mean) < len(test_mean) - 1 and test_mean[-1] < test_mean[np.argmax(test_mean)]:
            st.warning("‚ö†Ô∏è **Poss√≠vel overfitting**: a performance no conjunto de valida√ß√£o diminui ap√≥s um certo ponto.")
        else:
            st.success("‚úÖ **Sem sinais de overfitting claro**: a performance no teste acompanha a do treino.")
    
    except Exception as e:
        st.warning("N√£o foi poss√≠vel gerar a curva de perda. Modelo n√£o suporta valida√ß√£o direta.")
        
    # --- 6. PONTOS PARA DISCUSS√ÉO (L√∫dico e Educacional) ---
    st.markdown("### üí¨ Pontos para Reflex√£o")
    st.markdown("""
    #### üéØ 1. Precision vs Recall: Qual priorizar?
    - Se voc√™ √© **banco conservador**, quer alto **Recall** (capturar todos os inadimplentes).
    - Se quer **n√£o rejeitar bons clientes**, priorize **Precision**.
    - O **F1-Score** ajuda a equilibrar os dois.

    #### ‚ö†Ô∏è 2. Acur√°cia pode enganar!
    Se 95% dos clientes s√£o adimplentes, um modelo que diz "todos s√£o bons" ter√° 95% de acur√°cia... mas √© in√∫til.

    #### üìä 3. KS > 0.4 √© √≥timo, mas...
    ...verifique se o poder de separa√ß√£o √© est√°vel em subgrupos (idade, renda, regi√£o).

    #### üß© 4. E se o modelo for justo?
    Avalie se ele trata grupos sens√≠veis (g√™nero, ra√ßa) de forma equitativa. Isso √© **√©tica em IA**.

    #### üîÑ 5. E agora?
    Use essas m√©tricas para **ajustar o limiar de aprova√ß√£o** ou **melhorar o modelo**.
    """)

    # --- 7. RELAT√ìRIO DA AN√ÅLISE ---
    st.markdown("### üìÑ Relat√≥rio da An√°lise de Valida√ß√£o")
    st.info("Gere um resumo das m√©tricas e insights desta an√°lise para compartilhar ou documentar.")

    # Detecta overfitting apenas se a curva existir
    if modelo_tipo != "Regress√£o Log√≠stica" and 'test_mean' in locals():
        overfit_msg = ('Poss√≠vel overfitting detectado.'
                       if np.argmax(test_mean) < len(test_mean) - 1 and 
                          test_mean[-1] < test_mean[np.argmax(test_mean)]
                       else 'Sem sinais claros de overfitting.')
    else:
        overfit_msg = "N√£o analisado para Regress√£o Log√≠stica."
    # Prepara o conte√∫do do relat√≥rio
    relatorio_texto = f"""
    RELAT√ìRIO DE VALIDA√á√ÉO DO MODELO
    =================================
    
    üéØ Modelo: {modelo_tipo}
    üéØ Vari√°vel-alvo: {target}
    üéØ N√∫mero de vari√°veis preditoras: {len(features)}
    
    üìä M√âTRICAS PRINCIPAIS
    ----------------------
    Acur√°cia no Teste: {accuracy:.1%}
    Precision: {precision:.1%}
    Recall: {recall:.1%}
    F1-Score: {f1:.1%}
    AUC-ROC: {roc_auc:.2f}
    KS M√°ximo: {ks_max:.2f}
    Custo do Erro: R$ {custo_total:,.2f} ({percentual_perda:.2f}%)
    
    üîç INTERPRETA√á√ÉO
    ----------------
    - **Precision**: Entre os clientes classificados como inadimplentes, {precision:.1%} realmente s√£o.
    - **Recall**: O modelo identificou {recall:.1%} dos verdadeiros inadimplentes.
    - **AUC-ROC**: {'Excelente' if roc_auc > 0.9 else 'Bom' if roc_auc > 0.8 else 'Razo√°vel' if roc_auc > 0.7 else 'Fraco'} poder preditivo.
    - **KS**: {'Excelente' if ks_max > 0.4 else 'Bom' if ks_max > 0.3 else 'Moderado'} separa√ß√£o entre bons e maus.
    - **Custo do Erro**: Somat√≥rio do montante emprestado que resultou em inadimpl√™ncia. O percentual √© dado em rela√ß√£o ao total do valor emprestado.
    
    üìâ Overfitting
    -------------
    {overfit_msg}    
    
    üìÖ Data: {pd.Timestamp.now().strftime('%d/%m/%Y %H:%M')}
    """
    
    # Op√ß√µes de exporta√ß√£o
    export_option = st.radio("Escolha o formato de exporta√ß√£o:", ["Texto (.txt)", "Copiar para √°rea de transfer√™ncia"])
    
    if export_option == "Texto (.txt)":
        st.download_button(
            label="‚¨áÔ∏è Baixar Relat√≥rio (TXT)",
            data=relatorio_texto,
            file_name="relatorio_validacao_modelo.txt",
            mime="text/plain"
        )
    else:
        st.code(relatorio_texto, language="text")
        st.info("Voc√™ pode copiar o texto acima com o bot√£o no canto superior direito.")

    # --- NAVEGA√á√ÉO ---
    st.markdown("---")
    st.page_link("pages/8_‚öôÔ∏è_Aperfeicoamento.py", label="‚û°Ô∏è Ir para Aperfei√ßoamento", icon="‚öôÔ∏è")

if __name__ == "__main__":
    main()
