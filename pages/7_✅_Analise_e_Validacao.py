import streamlit as st
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, roc_auc_score
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

def main():
    st.title("âœ… AnÃ¡lise e ValidaÃ§Ã£o do Modelo")
    st.markdown("""
    Entenda **como o seu modelo se comporta na prÃ¡tica** com mÃ©tricas essenciais para credit scoring.  
    Aqui vocÃª vai aprender o que cada indicador significa â€” e por que ele importa.
    """)

    if 'modelo' not in st.session_state:
        st.warning("Nenhum modelo treinado! Construa um modelo primeiro.")
        st.page_link("pages/6_ğŸ¤–_Modelagem.py", label="â†’ Ir para Modelagem", icon="ğŸ¤–")
        return

    X_test = st.session_state.get('X_test')
    y_test = st.session_state.get('y_test')

    if X_test is None or y_test is None:
        st.error("Dados de teste nÃ£o disponÃ­veis. Treine o modelo novamente.")
        return

    model = st.session_state.modelo
    y_pred = model.predict(X_test)

    # Probabilidades (necessÃ¡rias para ROC e KS)
    try:
        y_proba = model.predict_proba(X_test)[:, 1]  # Probabilidade de classe 1 (inadimplente)
    except:
        st.warning("O modelo nÃ£o suporta `predict_proba`. Usando prediÃ§Ã£o direta.")
        y_proba = y_pred

    # --- 1. MATRIZ DE CONFUSÃƒO ---
    st.markdown("### ğŸ“Š Matriz de ConfusÃ£o")
    st.info("""
    Mostra os acertos e erros do modelo. Ajuda a entender **quem foi classificado corretamente**.
    - **VP (Verdadeiro Positivo)**: Inadimplente â†’ previsto como inadimplente âœ…  
    - **VN (Verdadeiro Negativo)**: Adimplente â†’ previsto como adimplente âœ…  
    - **FP (Falso Positivo)**: Adimplente â†’ previsto como inadimplente âŒ (rejeiÃ§Ã£o injusta)  
    - **FN (Falso Negativo)**: Inadimplente â†’ previsto como adimplente âŒ (pior erro: risco nÃ£o detectado)
    """)

    cm = confusion_matrix(y_test, y_pred)
    fig, ax = plt.subplots(figsize=(2.5, 2))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
                xticklabels=['Adimplente (0)', 'Inadimplente (1)'],
                yticklabels=['Adimplente (0)', 'Inadimplente (1)'])
    ax.set_ylabel('Real')
    ax.set_xlabel('Previsto')
    ax.set_title('Matriz de ConfusÃ£o')
    st.pyplot(fig)

    # --- 2. RELATÃ“RIO DE CLASSIFICAÃ‡ÃƒO AMIGÃVEL ---
    st.markdown("### ğŸ§  RelatÃ³rio de ClassificaÃ§Ã£o (Explicado)")
    st.info("""
    Abaixo, cada mÃ©trica Ã© explicada com base em um cenÃ¡rio de concessÃ£o de crÃ©dito.  
    Imagine que o modelo decide **a quem emprestar dinheiro**.
    """)

    # Calcula mÃ©tricas
    tn, fp, fn, tp = cm.ravel()
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    accuracy = (tp + tn) / (tp + tn + fp + fn)

    # Tabela de mÃ©tricas
    metricas_df = pd.DataFrame({
        "MÃ©trica": ["Precision", "Recall", "F1-Score", "AcurÃ¡cia"],
        "Valor": [f"{precision:.2%}", f"{recall:.2%}", f"{f1:.2%}", f"{accuracy:.2%}"],
        "ExplicaÃ§Ã£o": [
            "**Precision (PrecisÃ£o)**: Entre todos os clientes marcados como 'inadimplentes', quantos realmente sÃ£o? Alta precisÃ£o = poucos falsos positivos (nÃ£o rejeitamos bons clientes por engano).",
            "**Recall (Sensibilidade)**: Dos verdadeiros inadimplentes, quantos o modelo conseguiu identificar? Alto recall = poucos falsos negativos (capturamos mais risco).",
            "**F1-Score**: MÃ©dia harmÃ´nica entre Precision e Recall. Ã“timo quando queremos equilÃ­brio entre capturar risco e nÃ£o rejeitar bons clientes.",
            "**AcurÃ¡cia**: ProporÃ§Ã£o total de acertos. Pode ser enganosa se a base for desbalanceada (ex: 95% adimplentes)."
        ]
    })

    for _, row in metricas_df.iterrows():
        st.markdown(f"#### {row['MÃ©trica']} â†’ `{row['Valor']}`")
        st.markdown(row['ExplicaÃ§Ã£o'])
        st.markdown("---")

    # --- 3. CURVA ROC e AUROC ---
    st.markdown("### ğŸ“ˆ Curva ROC e AUC")
    st.info("""
    A **Curva ROC** mostra o trade-off entre **verdadeiros positivos (Recall)** e **falsos positivos** em diferentes limiares.
    - **AUC (Ãrea sob a curva)**: Quanto maior, melhor o modelo separa bons e maus pagadores.
    - **AUC > 0.7**: razoÃ¡vel | **> 0.8**: bom | **> 0.9**: excelente
    """)

    fpr, tpr, _ = roc_curve(y_test, y_proba)
    roc_auc = auc(fpr, tpr)

    fig, ax = plt.subplots(figsize=(6, 5))
    ax.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC (AUC = {roc_auc:.2f})')
    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='AleatÃ³rio (AUC = 0.5)')
    ax.set_xlim([0.0, 1.0])
    ax.set_ylim([0.0, 1.05])
    ax.set_xlabel('Taxa de Falsos Positivos (1 - Especificidade)')
    ax.set_ylabel('Taxa de Verdadeiros Positivos (Recall)')
    ax.set_title('Curva ROC')
    ax.legend(loc="lower right")
    st.pyplot(fig)

    st.metric("AUC (ROC)", f"{roc_auc:.2f}")

    # --- 4. TESTE KS (Kolmogorov-Smirnov) ---
    st.markdown("### ğŸ“Š EstatÃ­stica KS")
    st.info("""
    O **KS** mede a maior separaÃ§Ã£o entre a distribuiÃ§Ã£o acumulada de **bons** e **maus pagadores**.
    - **KS > 0.3**: bom poder de separaÃ§Ã£o
    - **KS > 0.4**: excelente
    Ã‰ uma mÃ©trica muito usada em crÃ©dito porque mostra claramente o poder discriminatÃ³rio do modelo.
    """)

    # DistribuiÃ§Ã£o acumulada
    thresholds = np.linspace(0, 1, 100)
    tpr_ks = [np.mean(y_proba[y_test == 1] >= th) for th in thresholds]
    fpr_ks = [np.mean(y_proba[y_test == 0] >= th) for th in thresholds]
    ks_values = [t - f for t, f in zip(tpr_ks, fpr_ks)]
    ks_max = max(ks_values)
    best_th = thresholds[np.argmax(ks_values)]

    fig, ax = plt.subplots(figsize=(6, 4))
    ax.plot(thresholds, tpr_ks, label='Bons (TPR)', color='green')
    ax.plot(thresholds, fpr_ks, label='Maus (FPR)', color='red')
    ax.plot(thresholds, ks_values, label='KS (diferenÃ§a)', color='blue', linestyle='--')
    ax.axvline(best_th, color='gray', linestyle=':', label=f'Limiar Ã³timo (KS): {best_th:.2f}')
    ax.set_xlabel('Limiar de decisÃ£o')
    ax.set_ylabel('Taxa acumulada')
    ax.set_title('AnÃ¡lise KS')
    ax.legend()
    st.pyplot(fig)

    st.metric("KS MÃ¡ximo", f"{ks_max:.2f}")
    st.caption(f"Limiar Ã³timo para KS: {best_th:.2f}")

    # --- 5. PONTOS PARA DISCUSSÃƒO (LÃºdico e Educacional) ---
    st.markdown("### ğŸ’¬ Pontos para ReflexÃ£o")
    st.markdown("""
    #### ğŸ¯ 1. Precision vs Recall: Qual priorizar?
    - Se vocÃª Ã© **banco conservador**, quer alto **Recall** (capturar todos os inadimplentes).
    - Se quer **nÃ£o rejeitar bons clientes**, priorize **Precision**.
    - O **F1-Score** ajuda a equilibrar os dois.

    #### âš ï¸ 2. AcurÃ¡cia pode enganar!
    Se 95% dos clientes sÃ£o adimplentes, um modelo que diz "todos sÃ£o bons" terÃ¡ 95% de acurÃ¡cia... mas Ã© inÃºtil.

    #### ğŸ“Š 3. KS > 0.4 Ã© Ã³timo, mas...
    ...verifique se o poder de separaÃ§Ã£o Ã© estÃ¡vel em subgrupos (idade, renda, regiÃ£o).

    #### ğŸ§© 4. E se o modelo for justo?
    Avalie se ele trata grupos sensÃ­veis (gÃªnero, raÃ§a) de forma equitativa. Isso Ã© **Ã©tica em IA**.

    #### ğŸ”„ 5. E agora?
    Use essas mÃ©tricas para **ajustar o limiar de aprovaÃ§Ã£o** ou **melhorar o modelo**.
    """)

    # --- NAVEGAÃ‡ÃƒO ---
    st.markdown("---")
    st.page_link("pages/8_âš™ï¸_Aperfeicoamento.py", label="â¡ï¸ Ir para AperfeiÃ§oamento", icon="âš™ï¸")

if __name__ == "__main__":
    main()
